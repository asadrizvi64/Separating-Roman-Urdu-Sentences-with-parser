# -*- coding: utf-8 -*-
"""a1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AYW5zN6l3nOYg4iToKiTvm2QkC00Keg

### Natural Language Processing: Assignment 1

##### This is demo program which give you idea about how to start  your assignment.
"""

## for installing UrduHack
!pip install urduhack[tf]

"""##### Reading File"""

import csv

with open('sent-test.txt', 'rt', encoding="utf-8") as f:
    reader = csv.reader(f)
    passage = list(reader)
f.close()
text = passage[0][0]
print(text)

"""##### Segmentation using UrduHack"""

import urduhack
from urduhack.tokenization import sentence_tokenizer

sentences = sentence_tokenizer(text)
sentences

len(sentences)

"""#### Reading Segmented File"""

with open('sent-segmented.txt', 'rt', encoding="utf-8") as f:
    reader = csv.reader(f)
    segmented = list(reader)
f.close()

seg_text = segmented[0][0]
sents = seg_text.split('۔')
len(sents)

def word_tokenize(text, corpus):
    tokens = []
    token = ""
    for character in text:
      # isalnum checks for special characters and letters and adds them into text, you need to create some rules for urdu script and then put a check here
        if character.isalnum():
            token += character
        else:
            if token:
                if token in corpus:
                    tokens.append(token)
                token = ""
    if token:
        if token in corpus:
            tokens.append(token)
    return tokens

import re

with open("word_test.txt", "r", encoding="utf-8") as file:
  text_ = file.read()
  # useless line below
  text = re.findall(r'\b\w+|\w+\b', text_)

with open("word-segmented.txt", "r", encoding="utf-8") as file:
  segmented_text = file.read()
  segmented_word = segmented_text.split()
  # the line below is useless as re library only works on english text
  segmented_words = re.findall(r'\b\w+|\w+\b', " ".join(segmented_word) )
  print(segmented_words)
  corpus = set(segmented_words)

test_words = word_tokenize(text, corpus)
print(test_words)

w_list = word_tokenize(seg_text,corpus)

w_list[0]

"""##### Your Sentence Segmentation Code"""

import re

def segment_sentences(text):
    sentences = []
    sentence = ""
    for character in text:
        sentence += "".join(character)
        if str(character) in ".؟":
            sentences.append(sentence.strip())
            sentence = ""
    return sentences

def segment_sentences_using_heuristics(text):
    text = re.sub("[0-9]+|[!@#$%^&*(),.?\":{}|<>]", "", text)
    text = text.replace("\n", "")
    text = text.strip()
    
    sentences = []
    sentence = ""
    for character in text:
        sentence += "".join(character)
        # add more conditions other than a question mark or fullstop, I think urdu doesnt have capitalization after eof tou you should look for trends in urdu script that will cause end of sentence
        if str(character) in ".؟":
            sentences.append(sentence.strip())
            sentence = ""
    return sentences

text = open("sent-test.txt").read()
print("---Using Character-based Tokenization---")
for sent in segment_sentences(text):
    print(sent)

print("\n---Using Heuristics---")
for sent in segment_sentences_using_heuristics(text):
    print(sent)

"""#### Compute Accuracy and Print Results"""

def compute_accuracy(ground_truth, predictions):
    assert len(ground_truth) == len(predictions), "Lists must have the same length."
    correct_predictions = 0
    for i in range(len(ground_truth)):
        if ground_truth[i] == predictions[i]:
            correct_predictions += 1
    accuracy = correct_predictions / len(ground_truth)
    return accuracy

def print_results(actual, predicted):
    accuracy = compute_accuracy(actual, predicted)
    print("Accuracy: {:.2f}%".format(accuracy * 100))
    for i, (a, p) in enumerate(zip(actual, predicted)):
        print("Example {}:".format(i + 1))
        print("Actual:")
        print("\n".join(a))
        print("Predicted:")
        print("\n".join(p))
        print("\n")

# read actual data from sent-segmented file
with open("sent-segmented.txt", "r", encoding="utf-8") as f:
    actual = [line.strip().split("\n") for line in f.readlines()]

# read predicted data from sent-test file
with open("sent-test.txt", "r", encoding="utf-8") as f:
    predicted = [line.strip().split("\n") for line in f.readlines()]

# send actual and predicted data to print_results function
print_results(actual, predicted)

from sklearn.metrics import f1_score

def evaluate_segmentation(actual_sentences, predicted_sentences):
    actual_boundaries = [0] + [i+1 for i, c in enumerate(actual_sentences) if c in ('.', '!', '?')] + [len(actual_sentences)]
    predicted_boundaries = [0] + [i+1 for i, c in enumerate(predicted_sentences) if c in ('.', '!', '?')] + [len(predicted_sentences)]
    y_true = []
    y_pred = []
    for i in range(len(actual_boundaries) - 1):
        y_true.extend([1] * (actual_boundaries[i+1] - actual_boundaries[i] - 1) + [0])
        y_pred.extend([1] * (predicted_boundaries[i+1] - predicted_boundaries[i] - 1) + [0])
    f1 = f1_score(y_true, y_pred)
    return f1

predicted_sentences = segment_sentences(predicted)
f1 = evaluate_segmentation(actual, predicted_sentences)
print("F1 score: ", f1)

def evaluate_word_tokenization(test_file, segmented_file):
    with open(test_file) as f:
        test_text = f.read()
    with open(segmented_file) as f:
        segmented_text = f.read()
    
    # split the text into words
    test_words = test_text.split()
    segmented_words = segmented_text.split()
    
    # check if the two arrays have the same number of elements
    if len(test_words) != len(segmented_words):
        raise ValueError("The test and segmented files have different number of elements")
    
    f1 = f1_score(test_words, segmented_words)
    return f1

print("F1 score: ", evaluate_word_tokenization("word_test.txt", "word-segmented.txt"))

import requests
from bs4 import BeautifulSoup

# The URL of the website to be scraped
url = "https://www.bbc.com/urdu"

# Make a GET request to the website
response = requests.get(url)

# Parse the HTML content of the page
soup = BeautifulSoup(response.content, "html.parser")

# Find the main-wrapper div element
main_wrapper = soup.find("div", id="main-wrapper")

# Find the div element with class "bbc-1ff36h2 eoodvxp0"
div_1 = main_wrapper.find("div", class_="bbc-1ff36h2 eoodvxp0")

# Find the div element with class "bbc-j6e0v4 e123m4vu4"
div_2 = div_1.find("div", class_="bbc-j6e0v4 e123m4vu4")

# Find the main element with role="main"
main2 = div_2.find_next_sibling("main", role="main")

# Find the div element with class "bbc-j6e0v4 e123m4vu4"
div_3 = main2.find("div", class_="bbc-j6e0v4 e123m4vu4")

# Extract the text from the main element
text = div_3.text.strip()

# Find the ul element with class "e2un9en2 bbc-klixw8 ebmt73l0"
ul_1 = div_2.find("ul", class_="e2un9en2 bbc-klixw8 ebmt73l0")

# Find the ul element with class "e2un9en1 bbc-1hwgxf ebmt73l0"
ul_2 = ul_1.find_next_sibling("ul", class_="e2un9en1 bbc-1hwgxf ebmt73l0")

# Find the main element with role="main"
main = ul_2.find_next_sibling("main", role="main")

# Extract the text from the main element
text = main.text.strip()

# Split the text into individual words
words = text.split()

# Create an empty list to store the vocabulary
vocabulary = []

# Loop over each word and add it to the vocabulary list if it's not already present
for word in words:
  if word not in vocabulary:
    vocabulary.append(word)

# Find the section element with labeledby="related-content-heading"
related_content = main.find("section", attrs={"labeledby": "related-content-heading"})

# Find the ul element with class "bbc-krv240 e1el4gw31"
ul_3 = related_content.find("ul", class_="bbc-krv240 e1el4gw31")

# Repeat the same process for the ul_3 element
text_2 = ul_3.text.strip()
words_2 = text_2.split()
for word in words_2:
  if word not in vocabulary:
    vocabulary.append(word)

# Loop over each ul element with class "bbc-krv240 e1el4gw31"
for ul in ul_3.find_all("ul", class_="bbc-krv240 e1el4gw31"):
  text = ul.text.strip()
  words = text.split()
  for word in words:
    if word not in vocabulary:
      vocabulary.append(word)

# Print the final vocabulary
print(vocabulary)

