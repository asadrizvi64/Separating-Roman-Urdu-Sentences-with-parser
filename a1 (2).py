# -*- coding: utf-8 -*-
"""a1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AYW5zN6l3nOYg4iToKiTvm2QkC00Keg

### Natural Language Processing: Assignment 1

##### This is demo program which give you idea about how to start  your assignment.
"""

## for installing UrduHack
!pip install urduhack[tf]

"""##### Reading File"""

import csv

with open('sent-test.txt', 'rt', encoding="utf-8") as f:
    reader = csv.reader(f)
    passage = list(reader)
f.close()
text = passage[0][0]
print(text)

"""##### Segmentation using UrduHack"""

import urduhack
from urduhack.tokenization import sentence_tokenizer

sentences = sentence_tokenizer(text)
sentences

len(sentences)

"""#### Reading Segmented File"""

with open('sent-segmented.txt', 'rt', encoding="utf-8") as f:
    reader = csv.reader(f)
    segmented = list(reader)
f.close()

seg_text = segmented[0][0]
sents = seg_text.split('۔')
len(sents)

def word_tokenize(text, corpus):
    tokens = []
    token = ""
    for character in text:
      # isalnum checks for special characters and letters and adds them into text, you need to create some rules for urdu script and then put a check here
        if character.isalnum():
            token += character
        else:
            if token:
                if token in corpus:
                    tokens.append(token)
                token = ""
    if token:
        if token in corpus:
            tokens.append(token)
    return tokens

import re

with open("word_test.txt", "r", encoding="utf-8") as file:
  text_ = file.read()
  # useless line below
  text = re.findall(r'\b\w+|\w+\b', text_)

with open("word-segmented.txt", "r", encoding="utf-8") as file:
  segmented_text = file.read()
  segmented_word = segmented_text.split()
  # the line below is useless as re library only works on english text
  segmented_words = re.findall(r'\b\w+|\w+\b', " ".join(segmented_word) )
  print(segmented_words)
  corpus = set(segmented_words)

test_words = word_tokenize(text, corpus)
print(test_words)

w_list = word_tokenize(seg_text,corpus)

w_list[0]

"""##### Your Sentence Segmentation Code

#### Compute Accuracy and Print Results
"""

def compute_accuracy(ground_truth, predictions):
    assert len(ground_truth) == len(predictions), "Lists must have the same length."
    correct_predictions = 0
    for i in range(len(ground_truth)):
        if ground_truth[i] == predictions[i]:
            correct_predictions += 1
    accuracy = correct_predictions / len(ground_truth)
    return accuracy

def print_results(actual, predicted):
    accuracy = compute_accuracy(actual, predicted)
    print("Accuracy: {:.2f}%".format(accuracy * 100))
    for i, (a, p) in enumerate(zip(actual, predicted)):
        print("Example {}:".format(i + 1))
        print("Actual:")
        print("\n".join(a))
        print("Predicted:")
        print("\n".join(p))
        print("\n")

# read actual data from sent-segmented file
with open("sent-segmented.txt", "r", encoding="utf-8") as f:
    actual = [line.strip().split("\n") for line in f.readlines()]

# read predicted data from sent-test file
with open("sent-test.txt", "r", encoding="utf-8") as f:
    predicted = [line.strip().split("\n") for line in f.readlines()]

# send actual and predicted data to print_results function
print_results(actual, predicted)

import re
import numpy as np

def split_urdu_sentences(text):
    # Split the merged text into individual sentences
    end_words = ["ہے", "ہیں", "تھے", "تھا", "گا", "گئے","تھی","دی","گے","نہیں","ہیں","تھا","گی","گی","گیا","ہوں","گئی", "دیا"]
    sentences = []
    current_sentence = ""
    for word in text.split():
        current_sentence += " " + word
        if word.endswith(tuple(end_words)):
            current_sentence += "۔"
            sentences.append(current_sentence.strip())
            current_sentence = ""
    if current_sentence:
        current_sentence += "۔"
        sentences.append(current_sentence.strip())
    return sentences


def tokenize_urdu_text(text, corpus):
    # Tokenize the words in the input text using a provided corpus
    stopwords = set(open("stopwords-ur.txt").read().split())
    tokens = []
    current_token = ""
    i = 0
    while i < len(text):
        current_char = text[i]
        current_token += current_char
        if current_token in corpus:
            if current_token not in stopwords:
                tokens.append(current_token)
            current_token = ""
        elif len(current_token) > 1:
            # Try to find bigger words by collecting multiple characters
            found_word = False
            for j in range(len(current_token)-1, 0, -1):
                if current_token[:j] in corpus and current_token[:j] not in stopwords:
                    tokens.append(current_token[:j])
                    current_token = current_token[j:]
                    found_word = True
                    break
            if not found_word:
                current_token = current_token[:-1]
                if current_token in corpus and current_token not in stopwords:
                    tokens.append(current_token)
                    current_token = current_char
                else:
                    current_token = current_char
        elif current_token == " ":
            current_token = ""
        i += 1
    if current_token:
        if current_token not in stopwords:
            tokens.append(current_token)
    return tokens

    
corpus= np.load('corpus.npy')
with open('word_test.txt', 'r', encoding='utf-8') as f:
    text = f.read()
sentences = split_urdu_sentences(text)
output = ""
for sentence in sentences:
    tokens = tokenize_urdu_text(sentence, corpus)
    output += " ".join(tokens) + "\n\n"

with open('output.txt', 'w', encoding='utf-8') as f:
    f.write(output)

import re

# Load the reference segmentation
with open('word-segmented.txt', 'r', encoding='utf-8') as f:
    ref_text = f.read()

# Load the output segmentation
with open('output.txt', 'r', encoding='utf-8') as f:
    out_text = f.read()

# Tokenize the reference and output segments
ref_tokens = re.findall(r'\w+', ref_text)
out_tokens = re.findall(r'\w+', out_text)

# Compute the true positives, false positives, and false negatives
tp = len(set(ref_tokens) & set(out_tokens))
fp = len(set(out_tokens) - set(ref_tokens))
fn = len(set(ref_tokens) - set(out_tokens))

# Compute precision, recall, and F1 score
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1 = 2 * precision * recall / (precision + recall)

print('Precision:', precision)
print('Recall:', recall)
print('F1 score:', f1)

