{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11_QtarKSjPf"
      },
      "source": [
        "### Natural Language Processing: Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S_p54xNSTq_"
      },
      "source": [
        "##### This is demo program which give you idea about how to start  your assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIhS278lWQOv",
        "outputId": "3214f901-239c-4d61-cac1-eb21d36cf2f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urduhack[tf]\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf2crf\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting tensorflow-datasets~=3.1\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click~=7.1 in /usr/local/lib/python3.8/dist-packages (from urduhack[tf]) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from urduhack[tf]) (2022.6.2)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.8/dist-packages (from urduhack[tf]) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.51.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (15.0.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (0.30.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (23.1.21)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (4.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.6.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (23.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.21.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.2->urduhack[tf]) (3.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (4.64.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (2.25.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (1.12.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (22.2.0)\n",
            "Collecting tensorflow-addons>=0.8.2\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.2->urduhack[tf]) (0.38.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (4.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (1.0.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack[tf]) (2.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack[tf]) (1.58.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.2->urduhack[tf]) (3.2.2)\n",
            "Installing collected packages: tensorflow-addons, tensorflow-datasets, tf2crf, urduhack\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.8.2\n",
            "    Uninstalling tensorflow-datasets-4.8.2:\n",
            "      Successfully uninstalled tensorflow-datasets-4.8.2\n",
            "Successfully installed tensorflow-addons-0.19.0 tensorflow-datasets-3.2.1 tf2crf-0.1.33 urduhack-1.1.1\n"
          ]
        }
      ],
      "source": [
        "## for installing UrduHack\n",
        "!pip install urduhack[tf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChiPrRsSwjz"
      },
      "source": [
        "##### Reading File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxjaBCgOWYKt",
        "outputId": "b02ff62a-f722-410c-926a-811589c40efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "with open('sent-test.txt', 'rt', encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    passage = list(reader)\n",
        "f.close()\n",
        "text = passage[0][0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTy04y0lcdY9"
      },
      "source": [
        "##### Segmentation using UrduHack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaG0ILX8cdY9",
        "outputId": "aaeafb38-014d-4c8e-9e81-3e47f2d6cf3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے',\n",
              " 'کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import urduhack\n",
        "from urduhack.tokenization import sentence_tokenizer\n",
        "\n",
        "sentences = sentence_tokenizer(text)\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAIh0JHocdY-",
        "outputId": "b7b2d76c-dc70-4b11-a85c-46d5c346ff3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(sentences) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gaElHuGS4Qb"
      },
      "source": [
        "#### Reading Segmented File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gnPi90zhegAM"
      },
      "outputs": [],
      "source": [
        "with open('sent-segmented.txt', 'rt', encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    segmented = list(reader)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVZ-eoXccdZA",
        "outputId": "1391b5bb-db98-40d8-f02d-f391729a9b77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "seg_text = segmented[0][0]\n",
        "sents = seg_text.split('۔')\n",
        "len(sents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def word_tokenize(text, corpus):\n",
        "    tokens = []\n",
        "    token = \"\"\n",
        "    for character in text:\n",
        "      # isalnum checks for special characters and letters and adds them into text, you need to create some rules for urdu script and then put a check here\n",
        "        if character.isalnum():\n",
        "            token += character\n",
        "        else:\n",
        "            if token:\n",
        "                if token in corpus:\n",
        "                    tokens.append(token)\n",
        "                token = \"\"\n",
        "    if token:\n",
        "        if token in corpus:\n",
        "            tokens.append(token)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "c0tDfYQ5iPvh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open(\"word_test.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  text_ = file.read()\n",
        "  # useless line below\n",
        "  text = re.findall(r'\\b\\w+|\\w+\\b', text_)\n",
        "\n",
        "with open(\"word-segmented.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  segmented_text = file.read()\n",
        "  segmented_word = segmented_text.split()\n",
        "  # the line below is useless as re library only works on english text\n",
        "  segmented_words = re.findall(r'\\b\\w+|\\w+\\b', \" \".join(segmented_word) )\n",
        "  print(segmented_words)\n",
        "  corpus = set(segmented_words)\n",
        "\n",
        "test_words = word_tokenize(text, corpus)\n",
        "print(test_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd8iOG7K_HVD",
        "outputId": "2407878b-cc41-46ca-cad5-6540a4152564"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['تجربہ', 'کار', 'ہندوستانی', 'آف', 'سپنر', 'روی', 'چندرن', 'ایشون', 'نے', 'آئندہ', 'ایشیاء', 'کپ', '2023ء', 'کی', 'غیر', 'یقینی', 'قسمت', 'پر', 'اپنی', 'رائے', 'کا', 'اظہار', 'کیا', 'ہے', 'جو', 'پاکستان', 'میں', 'ہونے', 'جا', 'رہا', 'ہے', 'اپنے', 'یوٹیوب', 'چینل', 'پر', 'بات', 'کرتے', 'ہوئے', 'روی', 'چندرن', 'ایشون', 'نے', 'کہا', 'کہ', 'اگر', 'پڑوسی', 'ملک', 'بھارت', 'ایشیا', 'کپ', '2023ء', 'میں', 'شرکت', 'کرنا', 'چاہتا', 'ہے', 'تو', 'مقام', 'تبدیل', 'کر', 'دینا', 'چاہیے', 'آف', 'سپنر', 'نے', 'کہا', 'کہ', 'انٹرنیشنل', 'کرکٹ', 'کونسل', 'آئی', 'سی', 'سی', 'نے', 'پاکستان', 'کو', 'ٹورنامنٹ', 'کی', 'میزبانی', 'کا', 'حق', 'دے', 'دیا', 'ہے', 'لیکن', 'بھارت', 'پاکستان', 'کا', 'دورہ', 'کرنے', 'کو', 'تیار', 'نہیں', 'روی', 'چندرن', 'ایشون', 'نے', 'بھی', '2023ء', 'میں', '50', 'اوور', 'کے', 'ورلڈ', 'کپ', 'کے', 'حوالے', 'سے', 'پاکستان', 'کرکٹ', 'بورڈ', 'پی', 'سی', 'بی', 'کے', 'حالیہ', 'بیان', 'کا', 'جواب', 'دیتے', 'ہوئے', 'کہا', 'میرے', 'خیال', 'میں', 'یہ', 'ممکن', 'نہیں', 'ہے', 'آف', 'سپنر', 'نے', 'مزید', 'کہا', 'کہ', 'پاکستان', 'نے', 'پہلے', 'بھارت', 'کا', 'دورہ', 'کرنے', 'سے', 'انکار', 'کر', 'دیا', 'تھا', 'لیکن', 'آخر', 'کار', 'وہ', 'میگا', 'ایونٹس', 'میں', 'شرکت', 'کے', 'لیے', 'بھارت', 'گئے', 'تھے', 'غور', 'طلب', 'ہے', 'کہ', 'ایشیا', 'کپ', 'کے', 'معاملے', 'پر', 'حال', 'ہی', 'میں', 'بحرین', 'میں', 'ہونے', 'والی', 'ایک', 'ہنگامی', 'میٹنگ', 'میں', 'غور', 'کیا', 'گیا', 'جہاں', 'وینیو', 'کے', 'بارے', 'میں', 'حتمی', 'فیصلہ', 'مارچ', 'تک', 'موخر', 'کر', 'دیا', 'گیا', 'بحرین', 'میں', 'ایشین', 'کرکٹ', 'کونسل', 'اے', 'سی', 'سی', 'کے', 'اجلاس', 'کے', 'بعد', 'بی', 'سی', 'سی', 'آئی', 'کے', 'حکام', 'نے', 'اعلان', 'کیا', 'کہ', 'بورڈ', 'نے', 'ایونٹ', 'کے', 'لیے', 'اپنی', 'ٹیم', 'پاکستان', 'نہ', 'بھیجنے', 'کا', 'فیصلہ', 'کیا', 'ہے', 'تاہم', 'پی', 'سی', 'بی', 'کے', 'حکام', 'نے', 'بھی', 'سخت', 'ردعمل', 'کا', 'اظہار', 'کرتے', 'ہوئے', 'کہا', 'ہے', 'کہ', 'وہ', 'اکتوبر', 'میں', 'بھارت', 'میں', 'ہونے', 'والے', 'ورلڈ', 'کپ', '2023ء', 'میں', 'شرکت', 'نہیں', 'کریں', 'گے', 'توشہ', 'خانہ', 'کیس', 'میں', 'عمران', 'خان', 'پر', 'فرد', 'جرم', 'عائد', 'نہ', 'ہو', 'سکی', 'سیشن', 'عدالت', 'نے', 'الیکشن', 'کمیشن', 'کی', 'چیئرمین', 'پی', 'ٹی', 'آئی', 'کیخلاف', 'فوجداری', 'مقدمے', 'کی', 'درخواست', 'پر', 'عمران', 'خان', 'کی', 'طبی', 'بنیادوں', 'پر', 'آج', 'حاضری', 'سے', 'استثن', 'ی', 'کی', 'درخواست', 'منظور', 'کر', 'لی', 'الیکشن', 'کمیشن', 'کی', 'جانب', 'سے', 'پی', 'ٹی', 'آئی', 'وکلا', 'کو', 'تصدیق', 'شدہ', 'کاپیاں', 'فراہم', 'نہیں', 'کی', 'گئیں', 'تصدیق', 'شدہ', 'کاپیاں', 'کی', 'فراہمی', 'کے', 'بعد', 'فرد', 'جرم', 'کی', 'اگلی', 'تاریخ', 'مقرر', 'کی', 'جائے', 'گی', 'تفصیلات', 'کے', 'مطابق', 'سیشن', 'عدالت', 'میں', 'الیکشن', 'کمیشن', 'کی', 'عمران', 'خان', 'کے', 'خلاف', 'فوجداری', 'مقدمے', 'کی', 'درخواست', 'پر', 'سماعت', 'ہوئی', 'جج', 'ظفر', 'اقبال', 'نے', 'درخواست', 'پر', 'سماعت', 'کی', 'عمران', 'خان', 'کی', 'جانب', 'سے', 'طبی', 'بنیادوں', 'پر', 'حاضری', 'سے', 'استثنی', 'کی', 'درخواست', 'دائر', 'کی', 'گئی', 'عدالت', 'نے', 'وکیل', 'سے', 'استفسار', 'کیا', 'کہ', 'کیا', 'مچلکے', 'جمع', 'کروا', 'دیئے', 'وکیل', 'گوہرعلی', 'خان', 'نے', 'کہا', 'کہ', 'عمران', 'خان', 'کے', 'ضمانتی', 'مچلکے', 'جمع', 'کروا', 'دیئے', 'جج', 'نے', 'استفسار', 'کیا', 'کہ', 'ایسے', 'حاضری', 'سے', 'استثن', 'ی', 'کی', 'درخواست', 'دائر', 'ہوتی', 'رہی', 'تو', 'فرد', 'جرم', 'کیسے', 'عائد', 'ہو', 'گی', 'وکیل', 'علی', 'بخاری', 'نے', 'بتایا', 'کہ', 'ہمیں', 'مصدقہ', 'کاپیاں', 'فراہم', 'نہیں', 'کی', 'گئیں', 'جج', 'نے', 'ہدایت', 'کی', 'کہ', 'تمام', 'ثبوتوں', 'کی', 'تصدیق', 'شدہ', 'کاپیاں', 'عدالت', 'اور', 'پی', 'ٹی', 'آئی', 'کو', 'فراہم', 'کریں', 'وکیل', 'الیکشن', 'کمیشن', 'نے', 'کہا', 'کہ', 'ہم', 'آج', 'ہی', 'کمپلینٹ', 'اور', 'ثبوتوں', 'کی', 'مصدقہ', 'کاپیاں', 'فراہم', 'کردیں', 'گے', 'وکیل', 'الیکشن', 'کمیشن', 'نے', 'کہا', 'کہ', 'عمران', 'خان', 'ابھی', 'تک', 'عدالت', 'کیوں', 'نہیں', 'آئے', 'جج', 'نے', 'استفسار', 'کیا', 'کہ', 'کیا', 'کہ', 'مجھے', 'ایک', 'تاریخ', 'بتا', 'دیں', 'عمران', 'خان', 'کب', 'عدالت', 'آئیں', 'گے', 'عمران', 'خان', 'کے', 'وکیل', 'نے', 'کہا', 'کہ', 'عمران', 'خان', 'کی', 'صحت', 'نے', 'اجازت', 'دی', 'تو', 'آئیں', 'گے', 'ڈاکٹرز', 'کی', 'ہدایت', 'پر', 'عمل', 'کر', 'رہے', 'ہیں', 'عدالت', 'نے', 'عمران', 'خان', 'کی', 'آج', 'حاضری', 'سے', 'استثنی', 'پر', 'فیصلہ', 'محفوظ', 'کر', 'لیا', 'عدالت', 'نے', 'طبی', 'بنیادوں', 'پر', 'عمران', 'خان', 'کی', 'آج', 'حاضری', 'سے', 'استثنی', 'کی', 'درخواست', 'منظور', 'کر', 'لی', 'چیئرمین', 'پی', 'ٹی', 'آئی', 'عمران', 'خان', 'پر', 'فردجرم', 'عائد', 'نہ', 'ہو', 'سکی', 'یاد', 'رہے', 'کہ', 'توشہ', 'خانہ', 'کیس', 'میں', 'عمران', 'خان', 'نے', 'سینئر', 'وکیل', 'خواجہ', 'حارث', 'کی', 'خدمات', 'حاصل', 'کر', 'لیں', 'خواجہ', 'حارث', 'سمیت', '4', 'وکلا', 'کے', 'وکالت', 'نامے', 'عدالت', 'میں', 'جمع', 'کرا', 'دیئے', 'گئے', 'ہیں', 'سابق', 'وفاقی', 'وزیر', 'مفتاح', 'اسماعیل', 'کا', 'کہنا', 'ہے', 'کہ', 'وزیر', 'خزانہ', 'اسحاق', 'ڈار', 'نے', 'پھر', 'وہی', 'حرکت', 'کی', 'جو', 'شوکت', 'ترین', 'نے', 'کی', 'تھی', 'انہوں', 'نے', 'آئی', 'ایم', 'ایف', 'سے', 'کیا', 'گیا', 'معاہدہ', 'توڑا', 'تھا', 'انہوں', 'نے', 'ہم', 'نیوز', 'کے', 'پروگرام', 'میں', 'گفتگو', 'کرتے', 'ہوئے', 'مزید', 'کہا', 'کہ', 'آئی', 'ایم', 'ایف', 'سے', 'معاہدہ', 'ہونے', 'کے', 'بعد', 'چیزیں', 'بہتر', 'ہوں', 'گی', 'پی', 'آئی', 'اے', 'اس', 'سال', '90', 'ارب', 'روپے', 'کا', 'نقصان', 'کرے', 'گی', 'لیکن', 'اگر', 'پی', 'آئی', 'اے', 'کی', 'نجکاری', 'ہو', 'گی', 'تو', '90', 'ارب', 'روپے', 'کا', 'نقصان', 'نہیں', 'ہوگا', 'مفتاح', 'اسماعیل', 'نے', 'کہا', 'کہ', 'میں', 'تو', 'چاہ', 'رہا', 'تھا', 'پہلے', 'دن', 'ہی', 'پٹرولیم', 'مصنوعات', 'کی', 'قیمتوں', 'میں', 'اضافہ', 'کیا', 'جائے', 'سیلاب', 'آنے', 'سے', 'پہلے', 'ہم', 'نے', 'ڈیفالٹ', 'رسک', 'کو', 'کم', 'کیا', 'تھا', 'شہباز', 'شریف', 'اگر', 'آج', 'وزیراعظم', 'نہ', 'ہوتے', 'تو', 'ملک', 'ڈیفالٹ', 'کی', 'طرف', 'جا', 'چکا', 'تھا', 'قبل', 'ازیں', 'ایک', 'بیان', 'میں', 'مفتاح', 'اسماعیل', 'نے', 'کہا', 'کہ', 'ہمسلم', 'لیگ', 'میں', 'کوئی', 'فارورڈ', 'بلاک', 'نہیں', 'بن', 'رہا', 'پاکستان', 'ڈیفالٹ', 'نہیںکرے', 'گا', 'پارٹی', 'کے', 'موجودہ', 'نائب', 'صدر', 'کو', 'پارٹی', 'کے', 'فیصلے', 'کرنے', 'کا', 'اختیار', 'حاصل', 'ہے', 'وہ', 'گزشتہ', 'روز', 'کراچی', 'میں', 'نیشنل', 'اسلامک', 'اکنامک', 'کانفرنس', 'سے', 'خطاب', 'کررہے', 'تھے', 'سابق', 'وزیر', 'خزانہ', 'نے', 'اپنے', 'خطاب', 'اور', 'میڈیا', 'سے', 'بات', 'چیت', 'کے', 'دوران', 'مسلم', 'لیگ', 'میںفارورڈ', 'بلاک', 'بننے', 'کی', 'خبروںکو', 'مسترد', 'کرتے', 'ہوئے', 'کہا', 'کہ', 'ایسی', 'اطلاعات', 'میںکوئی', 'صداقت', 'نہیںہے', 'شاہد', 'خاقان', 'عباسی', 'پہلے', 'ہی', 'کہہ', 'چکے', 'ہیںکہ', 'وہ', 'پارٹی', 'میںہی', 'ہیں', 'انہوںنے', 'اپنے', 'پارٹی', 'عہدہ', 'سے', 'استعفی', 'دیا', 'ہے', 'پارٹی', 'نہیں', 'چھوڑی', 'ہے', 'ملک', 'کی', 'معاشی', 'صورتحال', 'کے', 'حوالے', 'سے', 'مفتاح', 'اسماعیل', 'کا', 'کہنا', 'تھا', 'کہ', 'ملک', 'کے', 'ڈیفالٹ', 'ہونے', 'کا', 'کوئی', 'امکان', 'ہیں', 'ا', 'ئی', 'ایم', 'ایف', 'سے', 'ڈیل', 'ہوجائیگی', 'اور', 'معاملات', 'درست', 'ہوجائینگے', 'ان', 'کا', 'کہنا', 'تھا', 'کہ', 'اگرشوکت', 'ترین', 'فروری', 'میں', 'ا', 'ئی', 'ایم', 'ایف', 'کا', 'معاہدہ', 'نہ', 'توڑتے', 'تو', 'حالات', 'اچھے', 'ہوتے', 'پی', 'ٹی', 'ا', 'ئی', 'کی', 'حکومت', 'میں', '80', 'ارب', 'ڈالرکی', 'امپورٹ', 'ہوئی', 'ڈالرکوروک', 'کررکھنا', 'غلطی', 'تھی', 'اور', 'اس', 'لئے', 'ڈالرکی', 'قیمت', 'ا', 'ج', 'بلندترین', 'سطح', 'پرہے', 'ڈالر', 'کی', 'قدر', 'کو', 'پکڑ', 'کر', 'روکا', 'نہیں', 'جاسکتا', 'ڈالر', 'کا', 'فری', 'فلوٹ', 'رہنا', 'ہی', 'بہتر', 'ہے', 'غیرضروری', 'مشینری', 'کو', 'ہم', 'نے', 'امپورٹ', 'سے', 'روکا', 'تھا', 'ابھی', 'جو', 'پابندی', 'ہے', 'وہ', 'کچھ', 'اور', 'ہے', 'وزیر', 'داخلہ', 'رانا', 'ثناء', 'اللہ', 'کا', 'کہنا', 'ہے', 'کہ', 'عمران', 'خا', 'ن', 'نے', 'جیل', 'بھرو', 'تحریک', 'کا', 'اعلان', 'کیا', 'ہے', 'وہ', 'پہلے', 'بھی', 'اس', 'ہتھکنڈےمیں', 'ناکام', 'ہوئے', 'عمران', 'خا', 'ن', 'کو', 'معلوم', 'ہی', 'نہیں', 'کہ', 'جیل', 'میں', 'رہنا', 'کتنا', 'مشکل', 'ہے', 'میڈیا', 'رپورٹس', 'کے', 'مطابق', 'رانا', 'ثناء', 'اللہ', 'نے', 'اپنے', 'بیان', 'میں', 'کہا', 'کہ', 'عمران', 'خا', 'ن', 'کا', 'مقصد', 'سیاسی', 'افراتفری', 'ہے', 'وہ', 'اس', 'میں', 'ناکام', 'ہوں', 'گے', 'عمران', 'خان', 'اپنی', 'زندگی', 'کا', 'صرف', 'ایک', 'دن', 'جیل', 'میں', 'رہے', 'اے', 'پی', 'سی', 'میں', 'تمام', 'جماعتوں', 'کی', 'شرکت', 'کے', 'لیے', '2', 'دن', 'بڑھا', 'دیئے', 'ہیں', 'اے', 'پی', 'سی', 'میں', 'کسی', 'جماعت', 'کے', 'شامل', 'نہ', 'ہونے', 'سے', 'معاملہ', 'رک', 'نہیں', 'سکتا', 'وزیر', 'داخلہ', 'کا', 'کہنا', 'تھا', 'کہ', 'قومی', 'معاملے', 'کا', 'تمام', 'سیاسی', 'جماعتوں', 'کو', 'مل', 'کر', 'حل', 'نکالنا', 'ہو', 'گا', 'اے', 'پی', 'سی', 'میں', 'اتفاق', 'رائے', 'کے', 'بعد', 'حکومت', 'دہشت', 'گردوں', 'کے', 'خلاف', 'کارروائی', 'کا', 'فیصلہ', 'کرے', 'گی', 'قبل', 'ازیں', 'وفاقی', 'وزیر', 'ایاز', 'صادق', 'نے', 'کہا', 'کہ', 'پرویز', 'خٹک', 'اسد', 'قیصر', 'اور', 'اعجاز', 'شاہ', 'کو', 'اے', 'پی', 'سی', 'میں', 'شرکت', 'کی', 'دعوت', 'دی', 'ان', 'سے', 'کہا', 'کہ', 'وہ', 'یہ', 'دعوت', 'عمران', 'خان', 'تک', 'پہنچا', 'دیں', 'ایاز', 'صادق', 'نے', 'پی', 'ٹی', 'آئی', 'رہنما', 'اسد', 'قیصر', 'کے', 'بیان', 'پر', 'رد', 'عمل', 'دیتے', 'ہوئے', 'کہا', 'ہے', 'کہ', 'اے', 'پی', 'سی', 'میں', 'شرکت', 'کیلئے', 'کسی', 'کو', 'بھی', 'دعوت', 'نامہ', 'نہیں', 'بھجوایا', 'فون', 'کیے', 'یا', 'ملاقات', 'کر', 'کے', 'کانفرنس', 'میں', 'شرکت', 'کی', 'دعوت', 'دی', 'گئی', 'ہے', 'سابق', 'اسپیکر', 'قومی', 'اسمبلی', 'اسد', 'قیصر', 'رویوں', 'کی', 'بات', 'نہ', 'کریں', 'تو', 'اچھا', 'ہے', 'پی', 'ٹی', 'آئی', 'نے', 'ن', 'لیگ', 'پیپلز', 'پارٹی', 'کے', 'ساتھ', 'ایف', 'آئی', 'اے', 'اور', 'نیب', 'کے', 'ذریعے', 'کیا', 'کچھ', 'نہیں', 'کیا', 'جبکہ', 'تحریک', 'انصاف', 'کے', 'رہنما', 'اسد', 'قیصر', 'کا', 'کہنا', 'تھا', 'کہ', 'مجھے', 'حکومتی', 'نمائندوں', 'نے', 'فون', 'پر', 'اے', 'پی', 'سی', 'میں', 'شرکت', 'کی', 'دعوت', 'دی', 'کانفرنس', 'کی', 'دعوت', 'دینے', 'کا', 'یہ', 'بالکل', 'بھی', 'مناسب', 'طریقہ', 'نہیں', 'ہے', 'اے', 'پی', 'سی', 'میں', 'دعوت', 'دینا', 'کا', 'احسن', 'طریقہ', 'ہوتا', 'ہے', 'ہم', 'سمجھتے', 'ہیں', 'کہ', 'ملک', 'سنگین', 'بحرانوں', 'کا', 'شکار', 'ہے', 'اور', 'اس', 'وقت', 'یکجہتی', 'کی', 'ضرورت', 'ہے', 'حکومتی', 'رویہ', 'غیر', 'آئینی', 'ہے', 'اور', 'اس', 'ماحول', 'میں', 'یکجہتی', 'کا', 'سوال', 'ہی', 'پیدا', 'نہیں', 'ہوتا', 'حکومت', 'کو', 'سب', 'سے', 'پہلے', 'اپنے', 'رویے', 'میں', 'بہتری', 'لانا', 'ہو', 'گی', 'ہم', 'نے', 'اپنے', 'دور', 'میں', 'دہشت', 'گردی', 'پر', 'قابو', 'پانے', 'کیلئے', 'بہترین', 'حکمت', 'عملی', 'بنا', 'کر', 'امن', 'قائم', 'کیا']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "09BV-Ib-cdZB"
      },
      "outputs": [],
      "source": [
        "w_list = word_tokenize(seg_text,corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M5Kno-xLcdZB",
        "outputId": "61c099fd-20de-451d-b683-2d1d2e578fbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'خان'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "w_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnGpXBLicdZB"
      },
      "source": [
        "##### Your Sentence Segmentation Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ83jAJ3S-uR"
      },
      "source": [
        "#### Compute Accuracy and Print Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyFZPRIGlWNy",
        "outputId": "b958637e-2694-487b-a34e-1e84c9e52c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00%\n",
            "Example 1:\n",
            "Actual:\n",
            "عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے۔ کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا۔ میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا۔ ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔\n",
            "Predicted:\n",
            "عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(ground_truth, predictions):\n",
        "    assert len(ground_truth) == len(predictions), \"Lists must have the same length.\"\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(ground_truth)):\n",
        "        if ground_truth[i] == predictions[i]:\n",
        "            correct_predictions += 1\n",
        "    accuracy = correct_predictions / len(ground_truth)\n",
        "    return accuracy\n",
        "\n",
        "def print_results(actual, predicted):\n",
        "    accuracy = compute_accuracy(actual, predicted)\n",
        "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "    for i, (a, p) in enumerate(zip(actual, predicted)):\n",
        "        print(\"Example {}:\".format(i + 1))\n",
        "        print(\"Actual:\")\n",
        "        print(\"\\n\".join(a))\n",
        "        print(\"Predicted:\")\n",
        "        print(\"\\n\".join(p))\n",
        "        print(\"\\n\")\n",
        "\n",
        "# read actual data from sent-segmented file\n",
        "with open(\"sent-segmented.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    actual = [line.strip().split(\"\\n\") for line in f.readlines()]\n",
        "\n",
        "# read predicted data from sent-test file\n",
        "with open(\"sent-test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    predicted = [line.strip().split(\"\\n\") for line in f.readlines()]\n",
        "\n",
        "# send actual and predicted data to print_results function\n",
        "print_results(actual, predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def split_urdu_sentences(text):\n",
        "    # Split the merged text into individual sentences\n",
        "    end_words = [\"ہے\", \"ہیں\", \"تھے\", \"تھا\", \"گا\", \"گئے\",\"تھی\",\"دی\",\"گے\",\"نہیں\",\"ہیں\",\"تھا\",\"گی\",\"گی\",\"گیا\",\"ہوں\",\"گئی\", \"دیا\"]\n",
        "    sentences = []\n",
        "    current_sentence = \"\"\n",
        "    for word in text.split():\n",
        "        current_sentence += \" \" + word\n",
        "        if word.endswith(tuple(end_words)):\n",
        "            current_sentence += \"۔\"\n",
        "            sentences.append(current_sentence.strip())\n",
        "            current_sentence = \"\"\n",
        "    if current_sentence:\n",
        "        current_sentence += \"۔\"\n",
        "        sentences.append(current_sentence.strip())\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def tokenize_urdu_text(text, corpus):\n",
        "    # Tokenize the words in the input text using a provided corpus\n",
        "    stopwords = set(open(\"stopwords-ur.txt\").read().split())\n",
        "    tokens = []\n",
        "    current_token = \"\"\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        current_char = text[i]\n",
        "        current_token += current_char\n",
        "        if current_token in corpus:\n",
        "            if current_token not in stopwords:\n",
        "                tokens.append(current_token)\n",
        "            current_token = \"\"\n",
        "        elif len(current_token) > 1:\n",
        "            # Try to find bigger words by collecting multiple characters\n",
        "            found_word = False\n",
        "            for j in range(len(current_token)-1, 0, -1):\n",
        "                if current_token[:j] in corpus and current_token[:j] not in stopwords:\n",
        "                    tokens.append(current_token[:j])\n",
        "                    current_token = current_token[j:]\n",
        "                    found_word = True\n",
        "                    break\n",
        "            if not found_word:\n",
        "                current_token = current_token[:-1]\n",
        "                if current_token in corpus and current_token not in stopwords:\n",
        "                    tokens.append(current_token)\n",
        "                    current_token = current_char\n",
        "                else:\n",
        "                    current_token = current_char\n",
        "        elif current_token == \" \":\n",
        "            current_token = \"\"\n",
        "        i += 1\n",
        "    if current_token:\n",
        "        if current_token not in stopwords:\n",
        "            tokens.append(current_token)\n",
        "    return tokens\n",
        "\n",
        "    \n",
        "corpus= np.load('corpus.npy')\n",
        "with open('word_test.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "sentences = split_urdu_sentences(text)\n",
        "output = \"\"\n",
        "for sentence in sentences:\n",
        "    tokens = tokenize_urdu_text(sentence, corpus)\n",
        "    output += \" \".join(tokens) + \"\\n\\n\"\n",
        "\n",
        "with open('output.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(output)\n"
      ],
      "metadata": {
        "id": "Th3afT_YV9v9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Load the reference segmentation\n",
        "with open('word-segmented.txt', 'r', encoding='utf-8') as f:\n",
        "    ref_text = f.read()\n",
        "\n",
        "# Load the output segmentation\n",
        "with open('output.txt', 'r', encoding='utf-8') as f:\n",
        "    out_text = f.read()\n",
        "\n",
        "# Tokenize the reference and output segments\n",
        "ref_tokens = re.findall(r'\\w+', ref_text)\n",
        "out_tokens = re.findall(r'\\w+', out_text)\n",
        "\n",
        "# Compute the true positives, false positives, and false negatives\n",
        "tp = len(set(ref_tokens) & set(out_tokens))\n",
        "fp = len(set(out_tokens) - set(ref_tokens))\n",
        "fn = len(set(ref_tokens) - set(out_tokens))\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 score:', f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtqeWWldc3V3",
        "outputId": "3bf64d62-45e4-4262-bae3-4991109ee15f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.18023255813953487\n",
            "Recall: 0.06444906444906445\n",
            "F1 score: 0.09494640122511486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfXRhFSdeGyV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}